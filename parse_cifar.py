"""
Created Date: Apr 19, 2019

Created By: varunravivarma
-------------------------------------------------------------------------------

parse_cifar.py :
    Helper classes to parse the CIFAR-10 data

    1) Downloading the CIFAR-10 Dataset
    2) Unpickling the CIFAR-10 Dataset
    3) Converting image data into numpy arrays
    4) Loading class names
    5) Loading training data
    6) Loading test data

"""
import os
from urllib import request
import tarfile
import pickle
import numpy as np

# Data paths and variables

WEB_URL = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"

DATA_PATH = "./data/cifar_10/"

# Image variables (size, color channels and flattened image array size)

IMG_SIZE = 32

RGB_CHANNELS = 3

FLAT_IMG_SIZE = IMG_SIZE * IMG_SIZE * RGB_CHANNELS

# Size allocation variables

NUM_CLASSES = 10

NUM_TRAIN_FILES = 5

IMG_PER_FILE = 10000

# Functions for downloading, unpickling and parsing CIFAR-10 files

def _download_and_extract(web_url, download_dir):
    """
    Download and extract the CIFAR-10 dataset from the CS Toronto website if
    local data does not exist.

    Arguments:
        web_url: string, URL to the CIFAR-10 tar-ball file
        download_dir: string, Directory to save tar-ball file

    Returns:
        None
    """
    file_name = web_url.split('/')[-1]
    file_path = os.path.join(download_dir, file_name)

    if not os.path.exists(file_path):
        if not os.path.exists(download_dir):
            os.mkdir(download_dir)

        print("Downloading data.\nURL: {0}\nDestination Folder: {1}".format(web_url, file_path))
        file_path, _ = request.urlretrieve(web_url=web_url, filename=file_path)

        print("Download complete. Extracting files.")
        tarfile.open(name=file_path, mode="r:gz").extractall(download_dir)

        print("Extracted files to: {}".format(download_dir))

    else:
        print("File already exists at: {}".format(download_dir))

def _unpickle_files(file_name):
    """
    Unpickle data files and return data.

    Arguments:
        file_name: string, Name of pickle file

    Returns:
        pickle_data: dict, data from pickle file
    """
    file_path = os.path.join(DATA_PATH, "cifar-10-batches-py/", file_name)

    print("Loading file: {}".format(file_name))

    with open(file_path, mode="rb") as raw_data:
        pickle_data = pickle.load(raw_data, encoding='bytes')

    return pickle_data

def _convert_image_data(raw_images):
    """
    Converts image data retrieved from pickle file to numpy array.

    Arguments:
        raw_images: string, Raw image data from dictionary generated by unpacking pickle file

    Returns:
        img_array: np.array, Image data in format [image number, height, width, RGB channel]
    """
    image_data = (np.array(raw_images, dtype=float) / 255.0) \
                .reshape([-1, RGB_CHANNELS, IMG_SIZE, IMG_SIZE]) \
                .transpose([0, 2, 3, 1])

    return image_data

def _load_data(file_name):
    """
    Load data from pickle file and return converted images and class for image.

    Arguments:
        file_name: string, Pickle file to be loaded and processed

    Returns:
        images: np.array, Image data
        class_numbers: np.array, Class label as integer
    """
    raw_data = _unpickle_files(file_name=file_name)
    images = _convert_image_data(raw_images=raw_data[b'data'])
    class_numbers = np.array(raw_data[b'labels'])

    return images, class_numbers

def load_class_names():
    """
    Extract class names from meta file.

    Arguments:
        None

    Returns:
        class_names: list, class names
    """
    class_names_raw = _unpickle_files(file_name="batches.meta")[b'label_names']

    class_names = [x.decode('utf-8') for x in class_names_raw]

    return class_names

def load_train_data():
    """
    Load train data.

    Arguments:
        None

    Returns:
        train_images: np.array, Training image data
        train_class_numbers: np.array, Class numbers
        train_ohc_classes: np.array, One Hot Encoded Class numbers
    """

    _image_count = NUM_TRAIN_FILES * IMG_PER_FILE
    train_images = np.zeros(shape=[_image_count, IMG_SIZE, IMG_SIZE, RGB_CHANNELS], dtype=float)
    train_class_numbers = np.zeros(shape=[_image_count], dtype=int)

    batch_begin = 0
    batch_end = 0

    for i in range(NUM_TRAIN_FILES):
        batch_images, batch_classes = _load_data(file_name="data_batch_" + str(i + 1))
        batch_end = batch_begin + len(batch_images)

        train_images[batch_begin:batch_end, :] = batch_images
        train_class_numbers[batch_begin:batch_end] = batch_classes

        batch_begin = batch_end

    train_ohc_classes = np.eye(NUM_CLASSES, dtype=float)[train_class_numbers]

    return train_images, train_class_numbers, train_ohc_classes

def load_test_data():
    """
    Load test data.

    Arguments:
        None

    Returns:
        train_images: np.array, Testing image data
        test_class_numbers: np.array, Class numbers
        test_ohc_classes: np.array, One Hot Encoded Class numbers
    """

    test_images, test_class_numbers = _load_data(file_name="test_batch")

    test_ohc_classes = np.eye(NUM_CLASSES, dtype=float)[test_class_numbers]

    return test_images, test_class_numbers, test_ohc_classes

# TODO: Add train validation split method
# def train_val_split(val_size=0.3):
    """
    Create a validation set from training set.

    Arguments:
        val_size: float, Specifies percentage size of validation set to
            be created from train data

    Returns:
        train_images: np.array,
        train_classes:
        val_images:
        val_classes:
    """
